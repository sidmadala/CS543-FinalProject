{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the master dataset\n",
    "master_df = pd.read_csv('data/master_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Pre-processing the master_df to only contain data that we have images available for in the S3 bucket.  \n",
    "\n",
    "**Data manipulations performed:**\n",
    "1. Patients that had medicare or medicaid even in one entry will have medicare or medicaid for all entries. Because this is an association with SDOH features, it's fine to do this  \n",
    "\n",
    "2. Patients that had multiple ethnicities reported were purged so that we have 1 row for each patient/image\n",
    "\n",
    "3. NaN outcomes were replaced with 0. Ambigious outcomes were replaced with 0. Probably not the best way to go about it, but we can handle it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determine all the images we have access to\n",
    "## Setup boto3\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = \"cs543-final-project\"\n",
    "image_path = \"physionet.org/files/mimic-cxr-jpg/2.0.0/images/\"\n",
    "\n",
    "## Get list of files in bucket folder\n",
    "image_locs = []\n",
    "s3_bucket = s3.Bucket(bucket_name)\n",
    "for image_name in s3_bucket.objects.filter(Prefix=image_path):\n",
    "    image_locs.append(image_name.key[47:-4])\n",
    "\n",
    "\n",
    "## A lot of our images are duplications. One of the key duplication points is insurance.\n",
    "# imputation strategy, if at any point the patient used medicare or medicaid, all of the values for that patient\n",
    "# needs to be medicare or medicaid\n",
    "master_df.loc[master_df['subject_id'].isin(master_df.loc[master_df['insurance'] == 'Medicare', 'subject_id']), 'insurance'] = 'Medicare' \n",
    "master_df.loc[master_df['subject_id'].isin(master_df.loc[master_df['insurance'] == 'Medicaid', 'subject_id']), 'insurance'] = 'Medicaid' \n",
    "\n",
    "## Subset our dataset to only the images we have\n",
    "master_df = master_df.loc[master_df['dicom_id'].isin(image_locs)]\n",
    "master_df = master_df.drop_duplicates()\n",
    "master_df = master_df.loc[master_df['dicom_id'].isin(list(master_df.groupby('dicom_id')['dicom_id'].count().sort_values(ascending=False).loc[lambda x: x == 1].index))]\n",
    "master_df = master_df.reset_index(drop=True)\n",
    "master_df = master_df.replace(-1, 0).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class and Dataloaders\n",
    "\n",
    "We're creating the datsetclass in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import resize, pad\n",
    "from PIL import Image\n",
    "\n",
    "class MIMIC_CXR_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotation_file, outcome_list, image_prefix=\"data/images\", scaling_factor=9, tgrt_img_dims=[512,512], transforms=None):\n",
    "        self.annotation_file = annotation_file\n",
    "        self.image_prefix = image_prefix\n",
    "        self.outcome_list = outcome_list\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.tgrt_img_dims = tgrt_img_dims\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_file)\n",
    "\n",
    "    def __getitem__(self, image_name):\n",
    "        ## Get the patient outcomes\n",
    "        patient_info = self.annotation_file.loc[self.annotation_file['dicom_id'] == image_name]\n",
    "        label = patient_info[self.outcome_list].values[:]\n",
    "\n",
    "        ## Resize the input image by a factor of 9 --> want to preserve the image resolution\n",
    "        im = read_image(f\"{self.image_prefix}/{image_name}.jpg\")\n",
    "        if self.transforms is None:\n",
    "            ## Read the input image\n",
    "            dims = np.ceil(np.array(im.shape[1:])/self.scaling_factor).astype(int)\n",
    "            resized_image = resize(im, list(dims))[0]\n",
    "\n",
    "            ## Ensure that the image is evenly divisible\n",
    "            resized_image = resized_image[resized_image.shape[0]%2:, resized_image.shape[1]%2:]\n",
    "            dims = np.array(resized_image.shape)\n",
    "\n",
    "            ## Pad the image to the required target image size\n",
    "            transformed_image = pad(resized_image, list(np.flip(np.ceil((np.array(self.tgrt_img_dims) - dims)/2).astype(int)))).unsqueeze(0)\n",
    "        else:\n",
    "            im = im[0].numpy()\n",
    "#            im = np.dstack([im, im, im]).transpose(2,0,1)\n",
    "            im = np.dstack([im, im, im])\n",
    "            transformed_image = self.transforms(Image.fromarray(im))\n",
    "\n",
    "        return transformed_image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion', 'Lung Opacity', 'No Finding', 'Pleural Effusion', 'Pleural Other', 'Pneumonia', 'Pneumothorax', 'Support Devices']\n",
    "annotation_file = master_df[['dicom_id'] + outcome_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MIMIC_CXR_Dataset(annotation_file, outcome_list, image_prefix=\"data/images\", transforms=transforms.Compose([\n",
    "                                                                                                            transforms.RandomHorizontalFlip(),\n",
    "                                                                                                            transforms.RandomRotation(15),\n",
    "                                                                                                            transforms.Resize(256),\n",
    "                                                                                                            transforms.CenterCrop(256),\n",
    "                                                                                                            transforms.ToTensor(),\n",
    "                                                                                                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                                                                                        ]))\n",
    "\n",
    "\n",
    "train_dataLoader = torch.utils.data.DataLoader(train_dataset,batch_size=8, sampler=annotation_file['dicom_id'].values[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx, batch = next(enumerate(train_dataLoader))\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /home/ec2-user/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "## Load the model\n",
    "from torchvision import  models\n",
    "model = models.densenet121(pretrained=True)\n",
    "## Create the model's classification layer. We don't need a Sigmoid at the end of this because we'll incorporate it into the loss function\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, len(outcome_list)), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3b6d2671aae3ee82365a2db5b44c2bbc417dd0cc853ada8d6e3249c312b57bd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cs543-env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
